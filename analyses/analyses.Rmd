---
title: "Critical reanalysis of Vahey et al. (2015)"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

```

```{r}

# dependencies
library(tidyverse)
library(irr)
library(metafor)
library(pwr)
library(knitr)
library(kableExtra)

dir.create("models")
dir.create("plots")

# function to round all numeric vars in a data frame
round_df <- function(df, n_digits = 3) {
  df %>% mutate_if(is.numeric, round, digits = n_digits)
}

# apa format p value -----
apa_p_value <- function(p){
  p_formatted <- ifelse(p >= 0.001, paste("=", round(p, 3)),
                        ifelse(p < 0.001, "< .001", NA))
  p_formatted <- gsub(pattern = "0.", replacement = ".", x = p_formatted, fixed = TRUE)
  p_formatted
}

# add heterogeneity metrics to metafor forest plot
add_heterogeneity_metrics_to_forest <- function(fit) {
  bquote(paste("RE Model (", 
               italic('I')^"2", " = ", .(formatC(format(round(fit$I2, 1), nsmall = 1))),
               "%, ", italic('H')^"2", " = ", .(formatC(format(round(fit$H2, 1), nsmall = 1))), ")"))
}

# notation off
options(scipen = 999)

```

# Reproduction of Vahey et al (2015) 

## Power analyses

Vahey et al.'s reported meta effect size estimate was r = .45, 95% CI [.40, .54], 95% CR [.23, .67]. Using this effect size, they conducted a power analysis. They reported that, to detect a zero order correlation with 80% power, 29 participants were needed when using the ES or 37 if using the lower bound of the CI (following reccomendations by Perugini, Gallucci, & Costantini, 2014). I used the R package pwr to attempt to reproduce these values. 

- Minimum sample size using r = .45 = `r ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n)` participants to have 80% power, alpha = .05, one-tailed.
- Minimum sample size using lower CI r = .40 = `r ceiling(pwr.r.test(n = NULL, r = .40, sig.level = 0.05, power = 0.8, alternative = "greater")$n)` participants to have 80% power, alpha = .05, one-tailed.
- The results of Vahey et al.'s power analyses could therefore be computationally reproduced. 

However, Vahey et al.'s analytic choices here could be questioned: One-tailed correlations tests with alpha = .05 are very uncommon when reporting the significance of correlations, and regression analyses require two-sided testing. A two-tailed test with alpha = .05 would more correspond far more closely to modal research practices. I therefore recomputed sample size estimates using these parameters: 

- Minimum sample size using r = .45 = `r ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, **two**-tailed. 
- Minimum sample size using lower CI r = .40 = `r ceiling(pwr.r.test(n = NULL, r = .40, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, **two**-tailed.
- This suggested sample sizes are `r round(ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)/ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n), 2)*100 - 100`% higher than Vahey et al.'s reported estimates.

## Meta-analytic effect size 

The power analyses relied on the accuracy of the meta-analytic effect size. So, I then attempted to computationally reproduce the meta-analytic effect size from the effect sizes, confidence intervals and sample sizes reported in Vahey et al.'s forest plot and their description of their meta-analytic method in their manuscript. I also asked Vahey via email for additional details of their method and their analysis scripts, but but he declined to share them.

Information about implementation of Hunter & Schmidt-style meta-analysis in metafor from [here](http://www.metafor-project.org/doku.php/tips:hunter_schmidt_method).

```{r fig.height=5.5, fig.width=8}

data_vahey <- read.csv("../data/data_vahey_et_al_2015.csv")

data_vahey_forest_plot <- data_vahey %>%
  select(article = article_abbreviated, 
         yi = mean_r_forest_plot_numeric, 
         ni = n_forest_plot,
         ci_lower_forest_plot_numeric, 
         ci_upper_forest_plot_numeric) %>%
  na.omit() %>%
  # convert CIs to variance
  mutate(vi = ((ci_upper_forest_plot_numeric - ci_lower_forest_plot_numeric)/(1.96*2))^2) 

# fit model
fit_reproduced <- 
  rma(yi      = yi, 
      vi      = vi, 
      weights = ni,   # Hunter Schmidt method requires weighting by N
      method  = "HS",  # Hunter Schmidt method
      data    = data_vahey_forest_plot,
      slab    = article)

# save to disk for pdf plots
write_rds(fit_reproduced, "models/fit_reproduced.rds")

# make predictions 
predictions_reproduced <-
  predict(fit_reproduced, digits = 5) %>%
  as.data.frame() %>%
  gather(parameter, estimate) %>%
  round_df(2) %>%
  mutate(parameter = dplyr::recode(parameter,
                                   "pred" = "Meta r",
                                   "ci.lb" = "95% CI lower",
                                   "ci.ub" = "95% CI upper",
                                   "cr.lb" = "95% CR lower",
                                   "cr.ub" = "95% CR upper")) %>%
  filter(parameter != "se")

# plot
metafor::forest(fit_reproduced,
                xlab = substitute(paste("Pearson's ", italic('r'))),
                refline = 0,
                addcred = TRUE,
                mlab = add_heterogeneity_metrics_to_forest(fit_reproduced),
                xlim = c(-.75, 1.25),
                at = c(-.25, 0, .25, .5, .75, 1))
text(-.75, 17, "Reproduced from effect sizes in Vahey et al.'s forest plot", pos = 4)
text(1.25, 16.85, substitute(paste(italic('r'), " [95% CI]")), pos = 2)

# summarize results
meta_effect_reproduced <- 
  paste0("k = ", fit_reproduced$k, 
         ", r = ", predictions_reproduced$estimate[1],
         ", 95% CI [", predictions_reproduced$estimate[2], ", ", predictions_reproduced$estimate[3], "]", 
         ", 95% CR [", predictions_reproduced$estimate[4], ", ", predictions_reproduced$estimate[5], "]",
         ", p = ", signif(2*pnorm(-abs(fit_reproduced$zval)), digits = 1))  # exact p value from z score

meta_heterogeneity_reproduced <- 
  paste0("Q(df = ", fit_reproduced$k - 1, ") = ", round(fit_reproduced$QE, 2), 
         ", p ",       apa_p_value(fit_reproduced$QEp),
         ", tau^2 = ", round(fit_reproduced$tau2, 2), 
         ", I^2 = ",   round(fit_reproduced$I2, 2),
         ", H^2 = ",   round(fit_reproduced$H2, 2))

```

Meta effect: `r meta_effect_reproduced`.

Heterogeneity: `r meta_heterogeneity_reproduced`.

The results of Vahey et al.'s meta-analysis could therefore not be computationally reproduced using the data they reported in their forest plot and their descriptions of their analytic approach. Differences in the estimate of the meta effect size were similar but not identical. Differences in estimated heterogeneity were large (large heterogeneity in Vahey et al., no heterogeneity in my analysis).

## Weighted-average effect sizes reported in forest plot

Next, I attempted to reproduce the weighted-mean effect sizes reported in Vahey et al.'s forest plot from the individual effect sizes they reported in their supplementary materials. 

Vahey et al. reported that they weighted by degrees of freedom, and I do the same.

I noted that some of the confidence intervals in Vahey et al.'s forest plot are asymmetrical around the point estimate. This is uncommon and not accounted for by Vahey et al. detailing of how they calculated the effect sizes and their confidence intervals. However, I take them at face value as they are the most detailed data available to work from. 

```{r fig.height=4.5, fig.width=4.5}

# recalculate and compare
data_weighted_effect_sizes <- data_vahey %>%
  select(article = article_abbreviated, 
         r_supplementary, 
         df_supplementary) %>%
  group_by(article) %>%
  mutate(mean_df = mean(df_supplementary)) %>%
  ungroup() %>%
  mutate(r_weighted_by_df = r_supplementary*(df_supplementary/mean_df)) %>%
  group_by(article) %>%
  summarize(r_recalculated = round(mean(r_weighted_by_df), 2)) %>%
  # join with forest plot effect sizes
  left_join(data_vahey_forest_plot %>% select(article, r_forest_plot = yi), by = "article") %>%
  mutate(congruence = ifelse(r_forest_plot == r_recalculated, "Congruent", "Incongruent"),
         congruence = fct_relevel(congruence, "Incongruent", "Congruent"),
         difference = r_recalculated - r_forest_plot)

# plot
ggplot(data_weighted_effect_sizes, aes(r_forest_plot, r_recalculated)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(aes(color = congruence, shape = congruence), size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(0.35, 0.75) +
  ylim(0.35, 0.75) +
  xlab("Reported in Vahey et al.'s (2015)\nforest plot") +
  ylab("Recalculated from Vahey et al.'s (2015)\nsupplementary materials")

# table
data_weighted_effect_sizes %>%
  summarize(n_incongruent = sum(ifelse(congruence == "Incongruent", TRUE, FALSE)),
            percent_incongruent = round(mean(ifelse(congruence == "Incongruent", TRUE, FALSE)), 2)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_weighted_effect_sizes %>%
  filter(difference != 0) %>%
  select(article, r_recalculated, r_forest_plot, difference) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
  
```

The weighted effect sizes reported in Vahey et al.'s forest plot could not be computationally reproduced from the individual effect sizes and degrees of freedom reported in their supplementary materials. 

Many values were congruent, but a minority differed (k = 2 [13%]). Table includes those that differed.

## Extraction and conversion of individual effect sizes 

Only a subset of effect sizes were reextracted and reconverted to Pearson's r. This was for a variety of reasons (note that sometimes multiple reasons applied to a given effect size):

- Vahey et al. appear to have treated the $\eta^2_p$ effect size reported in original papers as if it was equivalent to $\eta^2$, which it is not: (a) $\eta^2$ has a relatively simpler mathematical transformation to Pearson's r, which Vahey et al. appear to have applied to $\eta^2_p$, which is not possible to convert to Pearson's r due to the fact that it is a partial correlation (e.g., Hooper et al., 2010; Hussey & Barnes-Holmes, 2012).
- In some cases, effect sizes reported in Vahey et al.'s supplementary materials did not refer to effect sizes that were reported in the original article (e.g., Timko et al., 2010; Study 1, correlation between overall D score	and DASS-total).
- In some cases, effect sizes referred to analyses where mean IRAP D scores were the Dependent Variable rather than the Independent Variable (e.g., Kosnes et al., 2013, Parling et al., 2012; Hussey et al., 2012; Timko et al., 2010; Study 1).
- Some were not reported in sufficient detail in the original paper to allow for the calculation of an effect size, and contact with the original authors did not result in data/result being provided (e.g., Parling et al., 2012).
- Vahey et al. included a large number of non-zero IRAP effects (i.e., a reaction time differential between block types) as effect sizes. However, it should be noted that the presence of an IRAP effect in isolation, without reference to an external criterion variable, by definition cannot provide evidence for the IRAP's (clinical) criterion validity. As such, these effects were not considered for reextraction and checking, given that (a) this extraction work was labor intensive, and (b) these effect sizes would not later be employed in the updated meta-analysis for the aforementioned reason.

It is important to note that my goal here was not to account for the accuracy of every effect size, but to (a) demonstrate the difficulty of even assessing how effect sizes were extracted and calculated, and (b) illustrating that a number of errors have been made in the effect sizes it was possible to reextract and reconvert (agnostic to whether the ones I have not managed to extract and convert are correct or incorrect). That is, highlighting errors in the extraction and conversion of some effect sizes is sufficient to make the point that issues exist.  

```{r fig.height=4.5, fig.width=4.5}

# get effect sizes
data_individual_effect_sizes <- read.csv("../data/data_reextracted_and_annotated.csv") %>%
  # consider only those effect sizes employed in vahey et al.'s meta-analysis
  filter(es_included_in_vahey_meta == TRUE) %>%
  # create an simplified effect size identifier
  group_by(article) %>%
  mutate(es_number = row_number()) %>%
  ungroup() %>%
  select(article, es_number, r_vahey, r_from_paper, n_from_paper) 

# table
data_individual_effect_sizes %>%
  summarize(n_reextracted = sum(!is.na(r_from_paper)),
            percent_reextracted = round(sum(!is.na(r_from_paper))/n(), 3)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# assess congruence
data_individual_effect_sizes_congruence <- data_individual_effect_sizes %>%
  na.omit() %>%
  mutate(congruence = ifelse(round(r_from_paper, 2) == round(r_vahey, 2), "Congruent", "Incongruent"),
         congruence = fct_relevel(congruence, "Incongruent", "Congruent"),
         difference = round(r_from_paper - r_vahey, 2))

# plot
ggplot(data_individual_effect_sizes_congruence, aes(r_vahey, r_from_paper)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(aes(color = congruence, shape = congruence), size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(0, 0.7) +
  ylim(0, 0.7) +
  xlab("Reported in Vahey et al.'s (2015)\nsupplementary materials") +
  ylab("Reextracted from original articles")

# table
data_individual_effect_sizes_congruence %>%
  summarize(n_incongruent = sum(ifelse(congruence == "Incongruent", TRUE, FALSE)),
            percent_incongruent = round(mean(ifelse(congruence == "Incongruent", TRUE, FALSE)), 2)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_individual_effect_sizes_congruence %>%
  filter(difference != 0) %>%
  select(article, r_vahey, r_from_paper, difference) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

Table includes those that differed.

## Critique of Vahey et al.'s inclusion and exclusion strategy 

**[notes on narrative, some coding still needed?]**

- how many ES were included by vahey, how many did they report considering?
- how many did i find.
- what were vahey's exclusion criteria?
- how many could be excluded using criterion 1: non-clinically relevant?
  - and, how many of vahey's could also be excluded as non-clinically relevant?
  - i.e., evidence of inconsistent application of inclusion/exclusion criteria.
- how many could be excluded using criterion 2: a priori predictable?
  - criterion seems inherently problematic, strong risk of post hoc bias. no blinding.
  - best I could do was re-rate all ES (only those scored as clinically relevant) for their predictability (rated by me and another researcher).
  - evidence from our ratings that many effects may have been predictable. seems very subjective - highlighting this point is useful at least.
  - evidence from Vahey et al.'s choices of what to include and not include seem less defensible however. Eg vahey et al 2009; comparisons between Nicholson et al. studies.

# New meta-analysis

- In attempting to determine whether Cahey et al.'s extracted effect sizes were accurately done, I realized that they made many mistakes of extraction or conversion, but more importantly many questionable decisions about which effect sizes they elected to include (e.g., presence of IRAP effects without reference to an external variable) or exclude (many effects that would seem to meet the criterion of being clinically relevant, but which were smaller in magnitude). I therefore elected to conduct a new meta-analysis using all clinically-relevant extracted effect sizes. I excluded the following from the extracted effect sizes:

- Mere IRAP effects without reference to an external variable that can provide criterion validity. By analogy, the fact that participants mean scores on on a self-report scale or a Stroop task tells us nothing about these measures' criterion validity, which is by definition assessed in relation to other external measures.  
- Effect sizes derived from analyses that employed IRAP effects as the DV. The final conclusion of the article is that the IRAP shows promise as a tool for clinical assessment, i.e., that it can be used to predict group membership. Predicting the mean IRAP effect on the basis of known groups is of relatively little utility. Moreover, this analytic mistake of confusing the IV and DV when attempting to provide evidence for a task's validity has been well documented elsewhere as a threat to research findings (e.g., Fried et al REF).
- Non-clinically relevant effects, using Vahey et al.'s definition (below) and scored by two independent raters, as in Vahey et al. I retained only effects that were rated by both raters as clinically relevant. 

From Vahey et al (2015): *"To be included within the current meta-analysis a given statistical effect must have described the co-variation of an IRAP effect with a corresponding clinically-focused criterion variable. To qualify as clinically-focused, the IRAP and criterion variables must have been deemed to target some aspect of a condition included in a major psychiatric diagnostic scheme such as the Diagnostic and Statistical Manual of Mental Disorders (DSM-5, 2013). ... The authors decided whether the responses measured by a given IRAP trial-type should co-vary with a specific criterion variable by consulting the relevant empirical literature. In the absence of such evidence, the authors strictly excluded even substantial statistical effects between IRAP effects and accompanying variables from the current meta-analysis."* Note that Vahey et al. did not provide citations of relevant literature that support the inclusion of any given effect.

```{r}

data_reextracted <- read.csv("../data/data_reextracted_and_annotated.csv") %>%
  # mark each effect as clinically relevant only if both raters scored it as such
  rowwise() %>%
  mutate(clinically_relevant = as.logical(max(c(clinically_relevant_criterion_rater_1,
                                                clinically_relevant_criterion_rater_2)))) %>%
  ungroup()

```

## Inter-rater reliability of inclusions

(i.e., of coding of effects as clinically relevant)

Vahey et al. do not report their rate of inter-rater reliability in the coding of effects as meeting inclusion/exclusion criteria.

I considered only the articles from which Vahey et al. extracted from, i.e., `r nrow(filter(data_reextracted, es_included_in_vahey_meta == TRUE))` effect sizes taken from `r nrow(distinct(data_reextracted, article))` articles, from an original pool of 46 articles. They did not report which or how many effect sizes were excluded, or the complete list of the original pool of articles. 

My reextraction found at least `r nrow(data_reextracted)` effect sizes in these `r nrow(distinct(data_reextracted, article))` articles. After exclusions, `r nrow(filter(data_reextracted, clinically_relevant == TRUE & problematic_analysis == FALSE))` effect sizes remained for inclusion in the new meta-analysis.

```{r}

data_raters <- data_reextracted %>%
  select(clinically_relevant_criterion_rater_1, clinically_relevant_criterion_rater_2)

data_raters %>%
  mutate(agreement = ifelse(clinically_relevant_criterion_rater_1 == clinically_relevant_criterion_rater_2, TRUE, FALSE)) %>%
  summarize(interrater_percent_agreement = round(mean(agreement, na.rm = TRUE), 1)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
  
kappa2(data_raters, "unweighted")

```

## Exclude non-clincially relevant and problematic

Details of the meta-analytic strategy, which used more up to date methods than Vahey et al.:

- Weighting by inverse variance rather than sample size, as sample size provides a poorer proxy of measurement error. this is now more standard practice in meta-analysis.
- Fishers r-to-z transformations to deal with ceiling effects on confidence intervals. 
- REML estimator function.
- Multilevel meta-analysis model rather than weighted-averaging of effect sizes (random intercept for source article; REF).

```{r fig.height=6, fig.width=5}

# exclusions and transformations
data_for_meta_new <- data_reextracted %>%
  filter(problematic_analysis == FALSE & clinically_relevant == TRUE) %>%
  mutate(variables = paste(irap_variable, criterion_variable, sep = "-")) %>%
  dplyr::select(article, variables, r_from_paper, n_from_paper, authors_include_bh) %>%
  na.omit() %>%
  escalc(measure = "ZCOR", 
         ri = r_from_paper, 
         ni = n_from_paper,
         data = ., 
         slab = paste(article, variables), 
         digits = 8) %>%
  rename(ni = n_from_paper) %>%
  dplyr::select(article, variables, yi, vi, ni, authors_include_bh) %>%
  na.omit() %>%
  group_by(article) %>%
  mutate(es_number = row_number()) %>%
  ungroup()

# fit 
fit_new <- 
  rma.mv(yi     = yi, 
         V      = vi, 
         random = ~ 1 | article, 
         method = "REML", 
         data   = data_for_meta_new,
         slab   = paste(article, es_number))

# save to disk for pdf plots
write_rds(fit_new, "models/fit_new.rds")

# make predictions 
predictions_new <-
  predict(fit_new, digits = 5) %>%
  as.data.frame() %>%
  gather(parameter, estimate) %>%
  mutate(parameter = dplyr::recode(parameter,
                                   "pred" = "Meta analysed r",
                                   "ci.lb" = "95% CI lower",
                                   "ci.ub" = "95% CI upper",
                                   "cr.lb" = "95% CR lower",
                                   "cr.ub" = "95% CR upper")) %>%
  filter(parameter != "se") %>%
  mutate(estimate = transf.ztor(estimate)) %>%
  round_df(2)

# caterpillar plot is more useful when k of effect sizes is large
metafor::forest(transf.ztor(data_for_meta_new$yi), 
                data_for_meta_new$vi,
                xlab = substitute(paste("Pearson's ", italic('r'))),
                subset = order(transf.ztor(data_for_meta_new$yi)),        ### order by size of yi
                slab = NA, 
                annotate = FALSE, ### remove study labels and annotations
                efac = 0,                  ### remove vertical bars at end of CIs
                pch = 19,                  ### changing point symbol to filled circle
                col = "gray40",            ### change color of points/CIs
                psize = 1.5,                 ### increase point size
                cex.lab = 1, 
                cex.axis = 1,   ### increase size of x-axis title/labels
                lty = c("solid","blank"),  ### remove horizontal line at top of plot
                xlim = c(-1.1, 1.1),    ### adjust horizontal plot region limits
                at = c(-1, -.5, 0, .5, 1))        
## draw points one more time to make them easier to see
points(sort(transf.ztor(data_for_meta_new$yi)), 
       nrow(data_for_meta_new):1, 
       pch = 19, 
       cex = 1)
## add meta effect size
addpoly(fit_new, 
        mlab = "", 
        annotate = FALSE, 
        addcred = TRUE,
        cex = 1)
## add text
text(-1, -2, "RE model", pos = 4, offset = 0, cex = 1)

# summarize results
meta_effect_new <- 
  paste0("k = ", fit_new$k, ", r = ", predictions_new$estimate[1],
         ", 95% CI [", predictions_new$estimate[2], ", ", predictions_new$estimate[3], "]",
         ", 95% CR [", predictions_new$estimate[4], ", ", predictions_new$estimate[5], "]",
         ", p = ", signif(2*pnorm(-abs(fit_new$zval)), digits = 1))  # exact p value from z score

# NB I2 is not trivial for multilevel models
meta_heterogeneity_new <-
  paste0("Q(df = ", fit_new$k - 1, ") = ", round(fit_new$QE, 2),
         ", p = ", ifelse(fit_new$QEp < 0.0001, "< .0001", as.character(round(fit_new$QEp, 4))),
         ", tau^2 = ", round(fit_new$tau2, 2))

```

Meta effect: `r meta_effect_new`.

Heterogeneity: `r meta_heterogeneity_new`.

- Note that this result is still known to be biased upward. Some papers were explicit that they reported only significant results in sufficient detail to calculate effect sizes (e.g., Parling et al., 2012).

## Updated power analyses

- Minimum sample size using r = `r predictions_new$estimate[1]` = `r ceiling(pwr.r.test(n = NULL, r = predictions_new$estimate[1], sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, *two*-tailed. 
- Minimum sample size using lower CI r = `r predictions_new$estimate[2]` = `r ceiling(pwr.r.test(n = NULL, r = predictions_new$estimate[2], sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, *two*-tailed.
- This suggested sample sizes are more than `r round(ceiling(pwr.r.test(n = NULL, r = predictions_new$estimate[1], sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)/ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n), 0)` times that recommended by Vahey et al.


