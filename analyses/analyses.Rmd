---
title: "Critical reanalysis of Vahey et al. (2015)"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# TODO

use field's equations to calculate a CR interval as they describe it

```{r, include=FALSE}

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

```

```{r}

# dependencies
library(tidyverse)
library(pwr)
library(irr)
library(metafor)
library(forestploter)
library(janitor)
library(greekLetters)
library(knitr)
library(kableExtra)

dir.create("models")
dir.create("plots")

# function to round all numeric vars in a data frame
round_df <- function(df, n_digits = 3) {
  df %>% mutate_if(is.numeric, janitor::round_half_up, digits = n_digits)
}

# apa format p value -----
apa_p_value <- function(p){
  p_formatted <- ifelse(p >= 0.001, paste("=", round_half_up(p, 3)),
                        ifelse(p < 0.001, "< .001", NA))
  p_formatted <- gsub(pattern = "0.", replacement = ".", x = p_formatted, fixed = TRUE)
  p_formatted
}

# heterogeneity strings
# heterogeneity_string <- function(fit){
#   #"Q(df=", fit$k - 1, ") = ", janitor::round_half_up(fit$QE, 2),
#   #        ", p ", ifelse(fit$QEp < 0.001, "< .001", paste0("= ", as.character(janitor::round_half_up(fit$QEp, 3)))),
#   #        ", ", 
#   if(!is.null(fit$I2)){
#     heterogeneity_string <- 
#       paste0("Heterogeneity: ", greekLetters::greeks("tau^2"), " = ", janitor::round_half_up(fit$tau2, 2),
#              ", ", greekLetters::greeks("I^2"), " = ", janitor::round_half_up(fit$I2, 2),
#              "%, ", greekLetters::greeks("H^2"), " = ",janitor::round_half_up(fit$H2, 2))
#   } else {
#     heterogeneity_string <- 
#       paste0("Heterogeneity: ", greekLetters::greeks("tau^2"), " = ", janitor::round_half_up(fit$tau2, 2))
#   }
#   return(heterogeneity_string)
# }
# 
# footnote   = heterogeneity_string(fit_new)

# notation off
options(scipen = 999)

# plot theme
custom_theme <- 
  forest_theme(base_size    = 10,
               ci_lty       = 1,
               ci_lwd       = 1.5,
               ci_Theight   = 0.2,
               summary_fill = "black",
               summary_col  = "black",
               footnote_cex = 0.8)

# get data
data_vahey <- read.csv("../data/data_vahey_et_al_2015.csv")

```

# Computational replication of original analylses

## Power analyses from meta analytic effect sizes reported in forest plot and text

Vahey et al.'s reported meta effect size estimate was r = .45, 95% CI [.40, .54], 95% CR [.23, .67]. Using this effect size, they conducted a power analysis. 

- They reported that: "based on the current meta-effect [r = .40], a sample size of 29 participants would provide a study with a statistical power of .80 when examining the statistical significance of first-order Pearson's r correlations between clinically-focused IRAP effects and corresponding criterion variables" (p.63); and 
- "Adopting a conservative approach in favour of controlling for overly optimistic publication biases, the most recent recommendation is to calculate sample size requirements not in terms of a given meta-effect, but rather in terms of the lower bound of its associated confidence interval (Perugini, Gallucci, & Costantini, 2014). Given that we obtained a confidence interval of (.40, .54) around the present meta-effect, Perugini et al.'s approach implies that a sample size of at least N = 37 would be required in order to achieve a statistical power of .80 when testing a continuous first-order correlation between a clinically-focused IRAP effect and a given criterion variable" (p.63).

I used the R package pwr to attempt to reproduce these values. 

- Minimum sample size using r = .45: `r ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n)` participants to have 80% power, alpha = .05, one-tailed. 
- Minimum sample size using lower CI r = .40: `r ceiling(pwr.r.test(n = NULL, r = .40, sig.level = 0.05, power = 0.8, alternative = "greater")$n)` participants to have 80% power, alpha = .05, one-tailed.
- The results of Vahey et al.'s power analyses could therefore be computationally reproduced *if* one uses the more liberal one-tailed test, which was not specified by Vahey et al. 

However, Vahey et al.'s analytic choices here could be questioned: One-tailed correlations tests with alpha = .05 are very uncommon when reporting the significance of correlations, and regression analyses require two-sided testing. A two-tailed test with alpha = .05 would more correspond far more closely to modal research practices. I therefore recomputed sample size estimates using these parameters: 

- Minimum sample size using r = .45: `r ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, **two**-tailed. 
- Minimum sample size using lower CI r = .40: `r ceiling(pwr.r.test(n = NULL, r = .40, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, **two**-tailed.
- This suggested sample sizes using more common assumptions are `r round_half_up(ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)/ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n), 2)*100 - 100`% higher than Vahey et al.'s reported estimates.

## Meta-analytic effect size and intervals from weighted average effect sizes reported in forest plot

The power analyses relied on the accuracy of the meta-analytic effect size. So, I then attempted to computationally reproduce the meta-analytic effect size from the effect sizes, confidence intervals, credibility intervals, and sample sizes reported in Vahey et al.'s forest plot and their description of their meta-analytic method in their manuscript. 

In an email, Vahey stated that they performed a Hunter & Schmidt meta analysis, following the guide of Field & Gillett (2010) and using their code.

On inspection, Field & Gillett (2010) make a distinction between the Hunter & Schmidt method, which is distinctive for reporting credibility intervals rather than confidence intervals, and the Hedges and colleagues’ method, which is distinctive for using Fisher's r-to-z transformations. 

Vahey et al. (2015) do not report applying any transformations to their data. However, Vahey et al.'s (2015) individual effect sizes in their forest plot have asymmetric confidence intervals. This implies that a transformation was performed. It is therefore possible that Vahey et al. combined the two methods (and code) provided by Field & Gillett (2010). I first fit a Hunter & Schmidt method, then a combined Hunter & Schmidt and Hedges' and colleagues method.  

### Hunter & Schmidt method using Field & Gillett's (2010) SPSS script

See "computational replication of meta-analysis via SPSS script" folder.

Summary of results:

```{r}

tibble(
  analysis = c("k studies", "Total N", "Effect size r", "CI lower", "CI upper", "CR lower", "CR upper"),
  vahey = c(15, 494, .45, .40, .54, .23, .67),
  reanalysis = c(15, 494, .4670, .1955, .7392, .4674, .4674)
)  %>%
  round_df(2) %>%
  mutate(diff = reanalysis - vahey,
         congruence = reanalysis == vahey) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

- Meta estimate *is not* the same as Vahey et al.'s forest plot.
- Meta confidence intervals *are not* same as Vahey et al.'s stated in text.
- Meta credibility/prediction intervals *are not* the same as Vahey et al.'s credibility intervals in their forest plot.

### Hunter & Schmidt method using metafor R package

Information about implementation of Hunter & Schmidt-style meta-analysis in metafor from [here](http://www.metafor-project.org/doku.php/tips:hunter_schmidt_method).

```{r}

total_n <- data_vahey %>%
  distinct(article, n_forest_plot) %>%
  summarize(total_n = sum(n_forest_plot, na.rm = TRUE)) %>%
  pull(total_n)

# calculate variances
data_recalculated_variance <- 
  escalc(measure = "COR", 
         ri      = mean_r_forest_plot_numeric, 
         ni      = n_forest_plot, 
         data    = data_vahey, 
         vtype   = "AV") %>%
  drop_na() %>%
  select(article, article_abbreviated,
         yi, vi, ni = n_forest_plot) %>%
  mutate(lower = yi - sqrt(vi)*1.96,
         upper = yi + sqrt(vi)*1.96)

# fit model
fit_reproduced <- 
  rma(yi      = yi, 
      vi      = vi, 
      weights = ni, # Hunter Schmidt method requires weighting by N
      method  = "HS", # Hunter Schmidt method
      data    = data_recalculated_variance,
      slab    = article_abbreviated)

predictions_reproduced <- 
  predict(fit_reproduced) %>%
  as.data.frame() %>%
  select(-se) 

predictions_reproduced %>%
  round_df(2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# plot
data_forest <- data_recalculated_variance %>%
  drop_na() %>%
  select(article_abbreviated,
         n = ni,
         r = yi,
         lower,
         upper) %>%
  # bind_rows(tibble(article_abbreviated = "Meta",
  #                  n = 35, # arbitrary number to make the diamond an appropriate size
  #                  r = predictions_reproduced$pred,
  #                  lower = predictions_reproduced$pi.lb,
  #                  upper = predictions_reproduced$pi.ub)) %>%
  bind_rows(tibble(article_abbreviated = c("Meta (confidence interval)", "Meta (credibility interval)"),
                   n = 35, # arbitrary number to make the diamond an appropriate size
                   r = c(predictions_reproduced$pred, predictions_reproduced$pred),
                   lower = c(predictions_reproduced$ci.lb, predictions_reproduced$pi.lb),
                   upper = c(predictions_reproduced$ci.ub, predictions_reproduced$pi.ub))) %>%
  mutate(size = n/sum(n),
         n = ifelse(str_detect(article_abbreviated, "Meta"), total_n, n),
         article_abbreviated = fct_relevel(article_abbreviated,
                                           "Carpenter et al. (2012)", 
                                           "Dawson et al. (2009)", 
                                           "Hooper et al, (2010)", 
                                           "Hussey & Barnes-Holmes (2012)", 
                                           "Kishita et al. (2014)", 
                                           "Kosnes et al. (2013)", 
                                           "Nicholson & Barnes-Holmes (2012a)", 
                                           "Nicholson & Barnes-Holmes (2012b)", 
                                           "Nicholson, Dempsey et al. (2013)", 
                                           "Nicholson, McCourt et al. (2013)", 
                                           "Parling et al. (2011)", 
                                           "Remue et al. (2013)", 
                                           "Timko et al. (2010, Study 1)", 
                                           "Vahey et al. (2009)", 
                                           "Vahey et al. (2010)",
                                           "Meta")) %>%
  mutate(` ` = paste(rep(" ", 30), collapse = " ")) %>%
  select(Article = article_abbreviated, ` `, r, lower, upper, n, size) %>%
  round_df(2)

p_reproduced <- 
  forestploter::forest(data       = select(data_forest, -size),
                       est        = data_forest$r,
                       lower      = data_forest$lower, 
                       upper      = data_forest$upper,
                       sizes      = 1/data_forest$size,
                       #is_summary = c(rep(FALSE, nrow(data_forest)-1), TRUE),
                       is_summary = c(rep(FALSE, nrow(data_forest)-2), TRUE, TRUE),
                       ci_column  = 2,
                       ref_line   = 0,
                       theme      = custom_theme,
                       xlim       = c(-0.3, 1.1),
                       ticks_at   = c(-.2, 0, .2, .4, .6, .8, 1.0))

p_reproduced

ggplot2::ggsave(filename = "plots/forest plot Hunter & Schmidt method.pdf", 
                plot = p_reproduced,
                device = "pdf",
                width = 7.5, 
                height = 4.5, 
                units = "in")

```

- Individual estimates *are* the same as Vahey et al.'s forest plot.
- Individual estimates' confidence intervals *are not* the same as Vahey et al.'s forest plot.
- Meta estimate *is not* the same as Vahey et al.'s forest plot.
- Meta confidence intervals *are* same as Vahey et al.'s stated in text.
- Meta credibility/prediction intervals *are not* the same as Vahey et al.'s credibility intervals in their forest plot.

### Hunter & Schmidt method with Hedges' and colleagues transformations using metafor R package

Fisher's r-to-z transformation and back transformation, with HS estimator. No Overton (1998) transformation.

```{r}

# calculate variances
data_recalculated_variance_transformed <- 
  escalc(measure = "ZCOR", 
         ri      = mean_r_forest_plot_numeric, 
         ni      = n_forest_plot, 
         data    = data_vahey, 
         vtype   = "AV") %>%
  drop_na() %>%
  select(article, article_abbreviated,
         yi, vi, ni = n_forest_plot) %>%
  mutate(lower = yi - sqrt(vi)*1.96,
         upper = yi + sqrt(vi)*1.96)

# fit model
fit_reproduced_transformed <- 
  rma(yi      = yi, 
      vi      = vi, 
      weights = ni, # Hunter Schmidt method requires weighting by N
      method  = "HS", # Hunter Schmidt method
      data    = data_recalculated_variance_transformed,
      slab    = article_abbreviated)

predictions_reproduced_transformed <- 
  predict(fit_reproduced_transformed) %>%
  as.data.frame() %>%
  select(-se)

predictions_reproduced_transformed %>%
  mutate_all(transf.ztor) %>%
  round_df(2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# plot
data_forest_transformed <- data_recalculated_variance_transformed %>%
  select(article_abbreviated,
         n = ni,
         r = yi,
         lower,
         upper) %>%
  # bind_rows(tibble(article_abbreviated = "Meta",
  #                  n = 35, # arbitrary number to make the diamond an appropriate size
  #                  r = predictions_reproduced_transformed$pred,
  #                  lower = predictions_reproduced_transformed$pi.lb,
  #                  upper = predictions_reproduced_transformed$pi.ub)) %>%
  bind_rows(tibble(article_abbreviated = c("Meta (confidence interval)", "Meta (credibility interval)"),
                   n = 35, # arbitrary number to make the diamond an appropriate size
                   r = c(predictions_reproduced_transformed$pred, predictions_reproduced_transformed$pred),
                   lower = c(predictions_reproduced_transformed$ci.lb, predictions_reproduced_transformed$pi.lb),
                   upper = c(predictions_reproduced_transformed$ci.ub, predictions_reproduced_transformed$pi.ub))) %>%
  mutate(r = transf.ztor(r),
         lower = transf.ztor(lower),
         upper = transf.ztor(upper)) %>%
  mutate(size = n/sum(n),
         n = ifelse(str_detect(article_abbreviated, "Meta"), total_n, n),
         article_abbreviated = fct_relevel(article_abbreviated,
                                           "Carpenter et al. (2012)", 
                                           "Dawson et al. (2009)", 
                                           "Hooper et al, (2010)", 
                                           "Hussey & Barnes-Holmes (2012)", 
                                           "Kishita et al. (2014)", 
                                           "Kosnes et al. (2013)", 
                                           "Nicholson & Barnes-Holmes (2012a)", 
                                           "Nicholson & Barnes-Holmes (2012b)", 
                                           "Nicholson, Dempsey et al. (2013)", 
                                           "Nicholson, McCourt et al. (2013)", 
                                           "Parling et al. (2011)", 
                                           "Remue et al. (2013)", 
                                           "Timko et al. (2010, Study 1)", 
                                           "Vahey et al. (2009)", 
                                           "Vahey et al. (2010)",
                                           "Meta")) %>%
  mutate(` ` = paste(rep(" ", 30), collapse = " ")) %>%
  select(Article = article_abbreviated, ` `, r, lower, upper, n, size) %>%
  round_df(2)

p_reproduced_transformed <- 
  forestploter::forest(data       = select(data_forest_transformed, -size),
                       est        = data_forest_transformed$r,
                       lower      = data_forest_transformed$lower, 
                       upper      = data_forest_transformed$upper,
                       sizes      = 1/data_forest_transformed$size,
                       #is_summary = c(rep(FALSE, nrow(data_forest)-1), TRUE),
                       is_summary = c(rep(FALSE, nrow(data_forest)-2), TRUE, TRUE),
                       ci_column  = 2,
                       ref_line   = 0,
                       theme      = custom_theme,
                       xlim       = c(-0.3, 1.1),
                       ticks_at   = c(-.2, 0, .2, .4, .6, .8, 1.0))

p_reproduced_transformed

ggplot2::ggsave(filename = "plots/forest plot Hunter & Schmidt method with Fisher's r-to-z transformations.pdf", 
                plot = p_reproduced_transformed,
                device = "pdf",
                width = 7.5, 
                height = 4.5, 
                units = "in")

```

- Individual estimates *are* the same as Vahey et al.'s forest plot.
- Individual estimates' confidence intervals *are* the same as Vahey et al.'s forest plot.
- Meta estimate *is not* the same as Vahey et al.'s forest plot.
- Meta confidence intervals *are* same as Vahey et al.'s stated in text.
- Meta credibility/prediction intervals *are not* the same as Vahey et al.'s credibility intervals in their forest plot.

### Summary

I report three attempts to reproduce Vahey et al.'s results using (a) Field & Gillett's (2010) SPSS script for a Hunter & Schmidt style meta-analysis, which Vahey referred me to use to reproduce their results, (b) a close implementation of the same Hunter & Schmidt style meta-analysis in R using the well-established metafor package, and (c) the same Hunter& Schmidt meta-analysis in r/metafor adding the transformations Field & Gillett recommended for a different analytic strategy (i.e., Hedges' and colleagues use of Fisher's r-to-z transformations).

None of the three analytic strategies/implementations fully reproduced Vahey et al.'s results. 

All three return the same meta-estimate of effect size: r = .47, where Vahey et al reported r = .45.

Both of the metafor implementations produce the same confidence intervals as reported in Vahey et al, but Field & Gillett's SPSS script produces very different ones.

Only the analysis that included Fishers' r-to-z transformations can account for and replicate the asymmetric confidence intervals on individual effect sizes that Vahey et al. reported in their forest plot. At minimum, it appears that Vahey et al. employed (and did not report using) a mixture of the Hunter & Schmidt method and the Hedges' method of transformation. 

No implementation produced similar credibility intervals to Vahey et al.

## Weighted-average effect sizes reported in forest plot from individual effect sizes in supplementary materials

Next, I attempted to reproduce the weighted-mean effect sizes reported in Vahey et al.'s forest plot from the individual effect sizes they reported in their supplementary materials. 

Vahey et al. reported that they weighted by degrees of freedom, and I do the same.

I noted that some of the confidence intervals in Vahey et al.'s forest plot are asymmetrical around the point estimate. This is uncommon and not accounted for by Vahey et al. detailing of how they calculated the effect sizes and their confidence intervals. However, I take them at face value as they are the most detailed data available to work from. 

```{r fig.height=4.5, fig.width=4.5}

# recalculate and compare
data_weighted_effect_sizes <- data_vahey %>%
  select(article = article_abbreviated, 
         r_supplementary, 
         df_supplementary) %>%
  group_by(article) %>%
  mutate(mean_df = mean(df_supplementary)) %>%
  ungroup() %>%
  mutate(r_weighted_by_df = r_supplementary*(df_supplementary/mean_df)) %>%
  group_by(article) %>%
  summarize(r_recalculated_weighted_mean = mean(r_weighted_by_df)) %>%
  left_join(data_vahey %>% 
              select(article = article_abbreviated, 
                     mean_r_forest_plot_numeric) %>%
              drop_na(),
            by = "article") %>%
  mutate(congruence = ifelse(janitor::round_half_up(r_recalculated_weighted_mean, 2) == 
                               janitor::round_half_up(mean_r_forest_plot_numeric, 2), "Congruent", "Incongruent"),
         congruence = fct_relevel(congruence, "Incongruent", "Congruent"),
         difference = r_recalculated_weighted_mean - mean_r_forest_plot_numeric)

# plot
ggplot(data_weighted_effect_sizes, aes(mean_r_forest_plot_numeric, r_recalculated_weighted_mean)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(0.35, 0.75) +
  ylim(0.35, 0.75) +
  xlab("Reported in Vahey et al.'s (2015)\nforest plot") +
  ylab("Recalculated from Vahey et al.'s (2015)\nsupplementary materials")

# table
data_weighted_effect_sizes %>%
  summarize(n_incongruent = sum(ifelse(congruence == "Incongruent", TRUE, FALSE)),
            percent_incongruent = round_half_up(mean(ifelse(congruence == "Incongruent", TRUE, FALSE)), 2)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_weighted_effect_sizes %>%
  filter(difference != 0) %>%
  select(article, r_recalculated_weighted_mean, mean_r_forest_plot_numeric, difference, congruence) %>%
  round_df(2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
  
```

```{r fig.height=4.5, fig.width=4.5}

# plot
data_weighted_effect_sizes %>%
  escalc(measure = "COR",
         ri = r_recalculated_weighted_mean,
         ni = n_forest_plot,
         data = .,
         slab = article,
         digits = 8) %>%
  ggplot(aes(ci_lower_forest_plot, yi - vi*1.96)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(-0.3, 0.7) +
  ylim(-0.3, 0.7) +
  xlab("Reported in Vahey et al.'s (2015)\nforest plot") +
  ylab("Recalculated from Vahey et al.'s (2015)\nsupplementary materials") +
  ggtitle("CI lower")

data_weighted_effect_sizes %>%
  mutate(r_recalculated_weighted_mean_adjusted = r_recalculated_weighted_mean - (r_recalculated_weighted_mean*(1-r_recalculated_weighted_mean^2)) / (2*(n_forest_plot-3))) %>%
  escalc(measure = "ZCOR",
         ri = r_recalculated_weighted_mean_adjusted,
         ni = n_forest_plot,
         data = .,
         slab = article,
         digits = 8) %>%
  ggplot(aes(ci_lower_forest_plot, yi - vi*1.96)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(aes(color = congruence, shape = congruence), size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(-0.3, 0.8) +
  ylim(-0.3, 0.8) +
  xlab("Reported in Vahey et al.'s (2015)\nforest plot") +
  ylab("Recalculated from Vahey et al.'s (2015)\nsupplementary materials")

```

CORRISPONDANCE IS BETTER WITHOUT TRANSFORMATIONS. WHAT ARE THE IMPLICATIONS FOR DECODING WHAT VAHEY ACTUALLY REPORTED? IT SUGGESTS THAT WHAT HE REPORTS AS WEIGHTED MEANS IN HIS SUPPLEMENTARY 


The weighted effect sizes reported in Vahey et al.'s forest plot could not be computationally reproduced from the individual effect sizes and degrees of freedom reported in their supplementary materials. 

Many values were congruent, but a minority differed (k = 2 [13%]). Table includes those that differed.

## Extraction and conversion of individual effect sizes 

Only a subset of effect sizes were reextracted and reconverted to Pearson's r. This was for a variety of reasons (note that sometimes multiple reasons applied to a given effect size):

- Vahey et al. appear to have treated the $\eta^2_p$ effect size reported in original papers as if it was equivalent to $\eta^2$, which it is not: (a) $\eta^2$ has a relatively simpler mathematical transformation to Pearson's r, which Vahey et al. appear to have applied to $\eta^2_p$, which is not possible to convert to Pearson's r due to the fact that it is a partial correlation (e.g., Hooper et al., 2010; Hussey & Barnes-Holmes, 2012).
- In some cases, effect sizes reported in Vahey et al.'s supplementary materials did not refer to effect sizes that were reported in the original article (e.g., Timko et al., 2010; Study 1, correlation between overall D score	and DASS-total).
- Some were not reported in sufficient detail in the original paper to allow for the calculation of an effect size, and contact with the original authors did not result in data/result being provided (e.g., Parling et al., 2012).
- Vahey et al. included a large number of non-zero IRAP effects (i.e., a reaction time differential between block types) as effect sizes. However, it should be noted that the presence of an IRAP effect in isolation, without reference to an external criterion variable, by definition cannot provide evidence for the IRAP's (clinical) criterion validity. As such, these effects were not considered for reextraction and checking, given that (a) this extraction work was labor intensive, and (b) these effect sizes would not later be employed in the updated meta-analysis for the aforementioned reason.

It is important to note that my goal here was not to account for the accuracy of every effect size, but to (a) demonstrate the difficulty of even assessing how effect sizes were extracted and calculated, and (b) illustrating that a number of errors have been made in the effect sizes it was possible to reextract and reconvert (agnostic to whether the ones I have not managed to extract and convert are correct or incorrect). That is, highlighting errors in the extraction and conversion of some effect sizes is sufficient to make the point that issues exist.

```{r fig.height=4.5, fig.width=4.5}

# get effect sizes
data_individual_effect_sizes <- read.csv("../data/data_reextracted_and_annotated.csv") %>%
  # consider only those effect sizes employed in vahey et al.'s meta-analysis
  filter(es_included_in_vahey_meta == TRUE) %>%
  # create an simplified effect size identifier
  group_by(article) %>%
  mutate(es_number = row_number()) %>%
  ungroup() %>%
  select(article, es_number, r_vahey, r_from_paper, n_from_paper) 

# table
data_individual_effect_sizes %>%
  summarize(n_reextracted = sum(!is.na(r_from_paper)),
            percent_reextracted = round_half_up(sum(!is.na(r_from_paper))/n(), 3)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# assess congruence
data_individual_effect_sizes_congruence <- data_individual_effect_sizes %>%
  na.omit() %>%
  mutate(congruence = ifelse(round_half_up(r_from_paper, 2) == round_half_up(r_vahey, 2), "Congruent", "Incongruent"),
         congruence = fct_relevel(congruence, "Incongruent", "Congruent"),
         difference = round_half_up(r_from_paper - r_vahey, 2))

# plot
ggplot(data_individual_effect_sizes_congruence, aes(r_vahey, r_from_paper)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(aes(color = congruence, shape = congruence), size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(0, 0.7) +
  ylim(0, 0.7) +
  xlab("Reported in Vahey et al.'s (2015)\nsupplementary materials") +
  ylab("Reextracted from original articles")

# table
data_individual_effect_sizes_congruence %>%
  summarize(n_incongruent = sum(ifelse(congruence == "Incongruent", TRUE, FALSE)),
            percent_incongruent = round_half_up(mean(ifelse(congruence == "Incongruent", TRUE, FALSE)), 2)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_individual_effect_sizes_congruence %>%
  filter(difference != 0) %>%
  select(article, r_vahey, r_from_paper, difference) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

Table includes those that differed.

## Critique of Vahey et al.'s inclusion and exclusion strategy 

**[notes on narrative, some coding still needed?]**

- how many ES were included by vahey, how many did they report considering?
- how many did i find.
- what were vahey's exclusion criteria?
- how many could be excluded using criterion 1: non-clinically relevant?
  - and, how many of vahey's could also be excluded as non-clinically relevant?
  - i.e., evidence of inconsistent application of inclusion/exclusion criteria.
- how many could be excluded using criterion 2: a priori predictable?
  - criterion seems inherently problematic, strong risk of post hoc bias. no blinding.
  - best I could do was re-rate all ES (only those scored as clinically relevant) for their predictability (rated by me and another researcher).
  - evidence from our ratings that many effects may have been predictable. seems very subjective - highlighting this point is useful at least.
  - evidence from Vahey et al.'s choices of what to include and not include seem less defensible however. Eg vahey et al 2009; comparisons between Nicholson et al. studies.

# Extended analyses

## Meta-analysis including all effects meeting Vahey et al.'s own inclusion and exclusion criteria

- In attempting to determine whether Vahey et al.'s extracted effect sizes were accurately done, I realized that they made many mistakes of extraction or conversion, but more importantly many questionable decisions about which effect sizes they elected to include (e.g., presence of IRAP effects without reference to an external variable) or exclude (many effects that would seem to meet the criterion of being clinically relevant, but which were smaller in magnitude). I therefore elected to conduct a new meta-analysis using all clinically-relevant extracted effect sizes. I excluded the following from the extracted effect sizes:

- Mere IRAP effects without reference to an external variable that can provide criterion validity. By analogy, the fact that participants mean scores on on a self-report scale or a Stroop task tells us nothing about these measures' criterion validity, which is by definition assessed in relation to other external measures.  
- Effect sizes derived from analyses that employed IRAP effects as the DV. The final conclusion of the article is that the IRAP shows promise as a tool for clinical assessment, i.e., that it can be used to predict group membership. Predicting the mean IRAP effect on the basis of known groups is of relatively little utility. Moreover, this analytic mistake of confusing the IV and DV when attempting to provide evidence for a task's validity has been well documented elsewhere as a threat to research findings (e.g., Fried et al REF).
- Non-clinically relevant effects, using Vahey et al.'s definition (below) and scored by two independent raters, as in Vahey et al. I retained only effects that were rated by both raters as clinically relevant. 

From Vahey et al (2015): *"To be included within the current meta-analysis a given statistical effect must have described the co-variation of an IRAP effect with a corresponding clinically-focused criterion variable. To qualify as clinically-focused, the IRAP and criterion variables must have been deemed to target some aspect of a condition included in a major psychiatric diagnostic scheme such as the Diagnostic and Statistical Manual of Mental Disorders (DSM-5, 2013). ... The authors decided whether the responses measured by a given IRAP trial-type should co-vary with a specific criterion variable by consulting the relevant empirical literature. In the absence of such evidence, the authors strictly excluded even substantial statistical effects between IRAP effects and accompanying variables from the current meta-analysis."* Note that Vahey et al. did not provide citations of relevant literature that support the inclusion of any given effect.

```{r}

data_reextracted <- read.csv("../data/data_reextracted_and_annotated.csv") %>%
  # mark each effect as clinically relevant only if both raters scored it as such
  rowwise() %>%
  mutate(clinically_relevant = as.logical(max(c(clinically_relevant_criterion_rater_1,
                                                clinically_relevant_criterion_rater_2)))) %>%
  ungroup()

total_n <- data_reextracted %>%
  distinct(article, n_from_paper) %>%
  summarize(total_n = sum(n_from_paper)) %>%
  pull(total_n)

```

### Inter-rater reliability of inclusions

(i.e., of coding of effects as clinically relevant)

Vahey et al. do not report their rate of inter-rater reliability in the coding of effects as meeting inclusion/exclusion criteria.

I considered only the articles from which Vahey et al. extracted from, i.e., `r nrow(filter(data_reextracted, es_included_in_vahey_meta == TRUE))` effect sizes taken from `r nrow(distinct(data_reextracted, article))` articles, from an original pool of 46 articles. They did not report which or how many effect sizes were excluded, or the complete list of the original pool of articles. 

My reextraction found at least `r nrow(data_reextracted)` effect sizes in these `r nrow(distinct(data_reextracted, article))` articles. After exclusions, `r nrow(filter(data_reextracted, clinically_relevant == TRUE & problematic_analysis == FALSE))` effect sizes remained for inclusion in the new meta-analysis.

```{r}

data_raters <- data_reextracted %>%
  select(clinically_relevant_criterion_rater_1, clinically_relevant_criterion_rater_2)

data_raters %>%
  mutate(agreement = ifelse(clinically_relevant_criterion_rater_1 == clinically_relevant_criterion_rater_2, TRUE, FALSE)) %>%
  summarize(interrater_percent_agreement = round_half_up(mean(agreement, na.rm = TRUE), 1)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
  
kappa2(data_raters, "unweighted")

```

### New meta analysis

Excluding non-clinically relevant and problematic

Details of the meta-analytic strategy, which used more up to date methods than Vahey et al.:

- Weighting by inverse variance rather than sample size, as sample size provides a poorer proxy of measurement error. This is now more standard practice in meta-analysis.
- Fishers r-to-z transformations to deal with ceiling effects on confidence intervals. 
- REML estimator function.
- Multilevel meta-analysis model rather than weighted-averaging of effect sizes (random intercept for source article).

```{r fig.height=32, fig.width=7.5}

# exclusions and transformations
data_for_meta_new <- data_reextracted %>%
  filter(problematic_analysis == FALSE & clinically_relevant == TRUE) %>%
  mutate(variables = paste(irap_variable, criterion_variable, sep = "-")) %>%
  dplyr::select(article, variables, r_from_paper, n_from_paper, authors_include_bh) %>%
  na.omit() %>%
  escalc(measure = "ZCOR", 
         ri = r_from_paper, 
         ni = n_from_paper,
         data = ., 
         slab = paste(article, variables), 
         digits = 8) %>%
  rename(ni = n_from_paper) %>%
  dplyr::select(article, variables, yi, vi, ni, authors_include_bh) %>%
  na.omit() %>%
  group_by(article) %>%
  mutate(es_number = row_number(),
         article_abbreviated = paste(article, es_number)) %>%
  ungroup() %>%
  mutate(lower = yi - sqrt(vi)*1.96,
         upper = yi + sqrt(vi)*1.96)

# fit 
fit_new <- 
  rma.mv(yi     = yi, 
         V      = vi, 
         random = ~ 1 | article, 
         method = "REML", 
         data   = data_for_meta_new,
         slab   = article_abbreviated)

# save to disk for pdf plots
#write_rds(fit_new, "models/fit_new.rds")

predictions_new <- 
  predict(fit_new) %>%
  as.data.frame() %>%
  select(-se) 

predictions_new %>%
  mutate_all(transf.ztor) %>%
  round_df(2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# plot
data_forest_new <- data_for_meta_new %>%
  drop_na() %>%
  select(article_abbreviated,
         n = ni,
         r = yi,
         lower,
         upper) %>%
  # bind_rows(tibble(article_abbreviated = "Meta",
  #                  n = 35, # arbitrary number to make the diamond an appropriate size
  #                  r = predictions_new$pred,
  #                  lower = predictions_new$ci.lb,
  #                  upper = predictions_new$ci.ub)) %>%
  bind_rows(tibble(article_abbreviated = c("Meta (confidence interval)", "Meta (credibility interval)"),
                   n = 35, # arbitrary number to make the diamond an appropriate size
                   r = c(predictions_new$pred, predictions_new$pred),
                   lower = c(predictions_new$ci.lb, predictions_new$pi.lb),
                   upper = c(predictions_new$ci.ub, predictions_new$pi.ub))) %>%
  mutate(r = transf.ztor(r),
         lower = transf.ztor(lower),
         upper = transf.ztor(upper)) %>%
  mutate(size = n/sum(n),
         n = ifelse(str_detect(article_abbreviated, "Meta"), total_n, n),
         article_abbreviated = fct_relevel(article_abbreviated,
                                           "Carpenter et al. (2012) 1", "Carpenter et al. (2012) 10", 
                                           "Carpenter et al. (2012) 11", "Carpenter et al. (2012) 12", "Carpenter et al. (2012) 13", 
                                           "Carpenter et al. (2012) 14", "Carpenter et al. (2012) 15", "Carpenter et al. (2012) 16", 
                                           "Carpenter et al. (2012) 17", "Carpenter et al. (2012) 18", "Carpenter et al. (2012) 19", 
                                           "Carpenter et al. (2012) 2", "Carpenter et al. (2012) 20", "Carpenter et al. (2012) 3", 
                                           "Carpenter et al. (2012) 4", "Carpenter et al. (2012) 5", "Carpenter et al. (2012) 6", 
                                           "Carpenter et al. (2012) 7", "Carpenter et al. (2012) 8", "Carpenter et al. (2012) 9", 
                                           "Dawson et al. (2009) 1", "Dawson et al. (2009) 2", "Dawson et al. (2009) 3", 
                                           "Dawson et al. (2009) 4", "Dawson et al. (2009) 5", "Dawson et al. (2009) 6", 
                                           "Hussey & Barnes-Holmes (2012) 1", "Hussey & Barnes-Holmes (2012) 10", 
                                           "Hussey & Barnes-Holmes (2012) 11", "Hussey & Barnes-Holmes (2012) 12", 
                                           "Hussey & Barnes-Holmes (2012) 13", "Hussey & Barnes-Holmes (2012) 14", 
                                           "Hussey & Barnes-Holmes (2012) 15", "Hussey & Barnes-Holmes (2012) 16", 
                                           "Hussey & Barnes-Holmes (2012) 17", "Hussey & Barnes-Holmes (2012) 18", 
                                           "Hussey & Barnes-Holmes (2012) 19", "Hussey & Barnes-Holmes (2012) 2", 
                                           "Hussey & Barnes-Holmes (2012) 20", "Hussey & Barnes-Holmes (2012) 21", 
                                           "Hussey & Barnes-Holmes (2012) 22", "Hussey & Barnes-Holmes (2012) 23", 
                                           "Hussey & Barnes-Holmes (2012) 24", "Hussey & Barnes-Holmes (2012) 25", 
                                           "Hussey & Barnes-Holmes (2012) 26", "Hussey & Barnes-Holmes (2012) 27", 
                                           "Hussey & Barnes-Holmes (2012) 28", "Hussey & Barnes-Holmes (2012) 29", 
                                           "Hussey & Barnes-Holmes (2012) 3", "Hussey & Barnes-Holmes (2012) 30", 
                                           "Hussey & Barnes-Holmes (2012) 4", "Hussey & Barnes-Holmes (2012) 5", 
                                           "Hussey & Barnes-Holmes (2012) 6", "Hussey & Barnes-Holmes (2012) 7", 
                                           "Hussey & Barnes-Holmes (2012) 8", "Hussey & Barnes-Holmes (2012) 9", 
                                           "Nicholson & Barnes-Holmes (2012b) 1", "Nicholson & Barnes-Holmes (2012b) 10", 
                                           "Nicholson & Barnes-Holmes (2012b) 2", "Nicholson & Barnes-Holmes (2012b) 3", 
                                           "Nicholson & Barnes-Holmes (2012b) 4", "Nicholson & Barnes-Holmes (2012b) 5", 
                                           "Nicholson & Barnes-Holmes (2012b) 6", "Nicholson & Barnes-Holmes (2012b) 7", 
                                           "Nicholson & Barnes-Holmes (2012b) 8", "Nicholson & Barnes-Holmes (2012b) 9", 
                                           "Nicholson, Dempsey et al. (2014) 1", "Nicholson, Dempsey et al. (2014) 10", 
                                           "Nicholson, Dempsey et al. (2014) 11", "Nicholson, Dempsey et al. (2014) 12", 
                                           "Nicholson, Dempsey et al. (2014) 13", "Nicholson, Dempsey et al. (2014) 14", 
                                           "Nicholson, Dempsey et al. (2014) 15", "Nicholson, Dempsey et al. (2014) 16", 
                                           "Nicholson, Dempsey et al. (2014) 17", "Nicholson, Dempsey et al. (2014) 18", 
                                           "Nicholson, Dempsey et al. (2014) 19", "Nicholson, Dempsey et al. (2014) 2", 
                                           "Nicholson, Dempsey et al. (2014) 20", "Nicholson, Dempsey et al. (2014) 21", 
                                           "Nicholson, Dempsey et al. (2014) 22", "Nicholson, Dempsey et al. (2014) 3", 
                                           "Nicholson, Dempsey et al. (2014) 4", "Nicholson, Dempsey et al. (2014) 5", 
                                           "Nicholson, Dempsey et al. (2014) 6", "Nicholson, Dempsey et al. (2014) 7", 
                                           "Nicholson, Dempsey et al. (2014) 8", "Nicholson, Dempsey et al. (2014) 9", 
                                           "Nicholson, McCourt et al. (2013) 1", "Nicholson, McCourt et al. (2013) 10", 
                                           "Nicholson, McCourt et al. (2013) 2", "Nicholson, McCourt et al. (2013) 3", 
                                           "Nicholson, McCourt et al. (2013) 4", "Nicholson, McCourt et al. (2013) 5", 
                                           "Nicholson, McCourt et al. (2013) 6", "Nicholson, McCourt et al. (2013) 7", 
                                           "Nicholson, McCourt et al. (2013) 8", "Nicholson, McCourt et al. (2013) 9", 
                                           "Timko et al. (2010; Study 1) 1", "Timko et al. (2010; Study 1) 10", 
                                           "Timko et al. (2010; Study 1) 11", "Timko et al. (2010; Study 1) 12", 
                                           "Timko et al. (2010; Study 1) 13", "Timko et al. (2010; Study 1) 14", 
                                           "Timko et al. (2010; Study 1) 15", "Timko et al. (2010; Study 1) 16", 
                                           "Timko et al. (2010; Study 1) 17", "Timko et al. (2010; Study 1) 18", 
                                           "Timko et al. (2010; Study 1) 19", "Timko et al. (2010; Study 1) 2", 
                                           "Timko et al. (2010; Study 1) 20", "Timko et al. (2010; Study 1) 21", 
                                           "Timko et al. (2010; Study 1) 22", "Timko et al. (2010; Study 1) 23", 
                                           "Timko et al. (2010; Study 1) 24", "Timko et al. (2010; Study 1) 25", 
                                           "Timko et al. (2010; Study 1) 26", "Timko et al. (2010; Study 1) 27", 
                                           "Timko et al. (2010; Study 1) 28", "Timko et al. (2010; Study 1) 29", 
                                           "Timko et al. (2010; Study 1) 3", "Timko et al. (2010; Study 1) 30", 
                                           "Timko et al. (2010; Study 1) 31", "Timko et al. (2010; Study 1) 32", 
                                           "Timko et al. (2010; Study 1) 4", "Timko et al. (2010; Study 1) 5", 
                                           "Timko et al. (2010; Study 1) 6", "Timko et al. (2010; Study 1) 7", 
                                           "Timko et al. (2010; Study 1) 8", "Timko et al. (2010; Study 1) 9", 
                                           "Timko et al. (2010; Study 2) 1", "Timko et al. (2010; Study 2) 10", 
                                           "Timko et al. (2010; Study 2) 11", "Timko et al. (2010; Study 2) 12", 
                                           "Timko et al. (2010; Study 2) 2", "Timko et al. (2010; Study 2) 3", 
                                           "Timko et al. (2010; Study 2) 4", "Timko et al. (2010; Study 2) 5", 
                                           "Timko et al. (2010; Study 2) 6", "Timko et al. (2010; Study 2) 7", 
                                           "Timko et al. (2010; Study 2) 8", "Timko et al. (2010; Study 2) 9",
                                           #"Meta"
                                           "Meta (confidence interval)", 
                                           "Meta (credibility interval)")) %>%
  mutate(` ` = paste(rep(" ", 30), collapse = " ")) %>%
  select(Article = article_abbreviated, ` `, r, lower, upper, n, size) %>%
  mutate(r     = round_half_up(r, 2),
         lower = round_half_up(lower, 2),
         upper = round_half_up(upper, 2))

p_new <- 
  forestploter::forest(data       = select(data_forest_new, -size),
                       est        = data_forest_new$r,
                       lower      = data_forest_new$lower, 
                       upper      = data_forest_new$upper,
                       sizes      = 1/data_forest_new$size,
                       # is_summary = c(rep(FALSE, nrow(data_forest_new)-1), TRUE),
                       is_summary = c(rep(FALSE, nrow(data_forest_new)-2), TRUE, TRUE),
                       ci_column  = 2,
                       ref_line   = 0,
                       theme      = custom_theme,
                       xlim       = c(-0.7, 0.9),
                       ticks_at   = c(-.6, -.4, -.2, 0, .2, .4, .6, .8))

p_new

ggplot2::ggsave(filename = "plots/forest plot new multilevel model.pdf", 
                plot = p_new,
                device = "pdf",
                width = 7.5, 
                height = 32, 
                units = "in")

# summarize results
predictions_new_backtransformed <- predictions_new %>%
  mutate_all(transf.ztor) %>%
  round_df(2)

meta_effect_new <- 
  paste0("k = ", fit_new$k, ", r = ", predictions_new_backtransformed$pred,
         ", 95% CI [", predictions_new_backtransformed$ci.lb, ", ", predictions_new_backtransformed$ci.ub, "]",
         ", 95% CR [", predictions_new_backtransformed$pi.lb, ", ", predictions_new_backtransformed$pi.ub, "]",
         ", p = ", signif(2*pnorm(-abs(fit_new$zval)), digits = 1))  # exact p value from z score

```

Meta effect: `r meta_effect_new`.

### Power analyses

Less appropriate one-tailed:

- Minimum sample size using r = `r predictions_new_backtransformed$pred` = `r ceiling(pwr.r.test(n = NULL, r = predictions_new_backtransformed$pred, sig.level = 0.05, power = 0.8, alternative = "greater")$n)` participants to have 80% power, alpha = .05, *one*-tailed. 
- Minimum sample size using lower CI r = `r predictions_new_backtransformed$ci.lb` = `r ceiling(pwr.r.test(n = NULL, r = predictions_new_backtransformed$ci.lb, sig.level = 0.05, power = 0.8, alternative = "greater")$n)` participants to have 80% power, alpha = .05, *one*-tailed.
- This suggested sample sizes are more than `r round_half_up(ceiling(pwr.r.test(n = NULL, r = predictions_new_backtransformed$pred, sig.level = 0.05, power = 0.8, alternative = "greater")$n)/ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n), 0)` times that recommended by Vahey et al.

More appropriate two-tailed:

- Minimum sample size using r = `r predictions_new_backtransformed$pred` = `r ceiling(pwr.r.test(n = NULL, r = predictions_new_backtransformed$pred, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, *two*-tailed. 
- Minimum sample size using lower CI r = `r predictions_new_backtransformed$ci.lb` = `r ceiling(pwr.r.test(n = NULL, r = predictions_new_backtransformed$ci.lb, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, *two*-tailed.
- This suggested sample sizes are more than `r round_half_up(ceiling(pwr.r.test(n = NULL, r = predictions_new_backtransformed$pred, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)/ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n), 0)` times that recommended by Vahey et al.

# Conceptual critiques

## Inclusion strategy of effects that "could have been predicted"

## Analyses of the IRAP as the DV rather than the DV

Incompatible with the idea that the IRAP can be used for clinical assessment. This would require that IRAP tells us something about individuals' clinical status/group/location on a continuum/etc (i.e., IRAP effects -> individuals). Most of the effect sizes come from analyses that assume the opposite, i.e., that individuals' clinical status/group/location on a continuum/etc. can tell us about their IRAP effects (i.e., individuals -> IRAP effects). 

Even this new meta-analysis is mostly informative only for researchers wishing to study IRAP effects, rather than study individuals using the IRAP.

Similar conceptual criticisms have been made elsewhere. Schmall et al (2016) reported results from a very large study which concluded that hippocampal volume ‘robustly discriminate[d] MDD patients from healthy controls’ (p. 1). However, Fried & Kievit (2016) pointed out that this was not the case: depression status predicted a detectable difference in mean hippocampal volume, but hippocampal volume was very weakly associated with indivdiuals' depression status. They point out that Schmall et al had reversed their analysis' DV and IV when making conclusions. This error is equally applicable to Vahey et al: even if their meta analysis was computationally replicable, its conclusions would still not support their claim that the meta ansysis “demonstrates the potential of the IRAP as a tool for clinical assessment” (p. 64). Rather, it would demonstrate that known clinical status (etc) has potential as a tool for the assessment of IRAP scores.





