---
title: "Critical reanalysis of Vahey et al. (2015)"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# TODO

use field's equations to calculate a CR interval as they describe it

```{r, include=FALSE}

knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

```

```{r}

# dependencies
library(tidyverse)
library(pwr)
library(irr)
library(metafor)
library(forestploter)
library(janitor)
library(knitr)
library(kableExtra)

dir.create("models")
dir.create("plots")

# function to round all numeric vars in a data frame
round_df <- function(df, n_digits = 3) {
  df %>% mutate_if(is.numeric, janitor::round_half_up, digits = n_digits)
}

# apa format p value -----
apa_p_value <- function(p){
  p_formatted <- ifelse(p >= 0.001, paste("=", round(p, 3)),
                        ifelse(p < 0.001, "< .001", NA))
  p_formatted <- gsub(pattern = "0.", replacement = ".", x = p_formatted, fixed = TRUE)
  p_formatted
}

# add heterogeneity metrics to metafor forest plot
add_heterogeneity_metrics_to_forest <- function(fit) {
  bquote(paste("RE Model (", 
               italic('I')^"2", " = ", .(formatC(format(round(fit$I2, 1), nsmall = 1))),
               "%, ", italic('H')^"2", " = ", .(formatC(format(round(fit$H2, 1), nsmall = 1))), ")"))
}

# notation off
options(scipen = 999)

# plot theme
custom_theme <- 
  forest_theme(base_size    = 10,
               ci_lty       = 1,
               ci_lwd       = 1.5,
               ci_Theight   = 0.2,
               summary_fill = "black",
               summary_col  = "black")

# get data
data_vahey <- read.csv("../data/data_vahey_et_al_2015.csv")

```

# Computational replication of Vahey et al (2015) 

## Power analyses

Vahey et al.'s reported meta effect size estimate was r = .45, 95% CI [.40, .54], 95% CR [.23, .67]. Using this effect size, they conducted a power analysis. 

- They reported that: "based on the current meta-effect [r = .40], a sample size of 29 participants would provide a study with a statistical power of .80 when examining the statistical significance of first-order Pearson's r correlations between clinically-focused IRAP effects and corresponding criterion variables" (p.63); and 
- "Adopting a conservative approach in favour of controlling for overly optimistic publication biases, the most recent recommendation is to calculate sample size requirements not in terms of a given meta-effect, but rather in terms of the lower bound of its associated confidence interval (Perugini, Gallucci, & Costantini, 2014). Given that we obtained a confidence interval of (.40, .54) around the present meta-effect, Perugini et al.'s approach implies that a sample size of at least N = 37 would be required in order to achieve a statistical power of .80 when testing a continuous first-order correlation between a clinically-focused IRAP effect and a given criterion variable" (p.63).

I used the R package pwr to attempt to reproduce these values. 

- Minimum sample size using r = .45: `r ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n)` participants to have 80% power, alpha = .05, one-tailed. 
- Minimum sample size using lower CI r = .40: `r ceiling(pwr.r.test(n = NULL, r = .40, sig.level = 0.05, power = 0.8, alternative = "greater")$n)` participants to have 80% power, alpha = .05, one-tailed.
- The results of Vahey et al.'s power analyses could therefore be computationally reproduced *if* one uses the more liberal one-tailed test, which was not specified by Vahey et al. 

However, Vahey et al.'s analytic choices here could be questioned: One-tailed correlations tests with alpha = .05 are very uncommon when reporting the significance of correlations, and regression analyses require two-sided testing. A two-tailed test with alpha = .05 would more correspond far more closely to modal research practices. I therefore recomputed sample size estimates using these parameters: 

- Minimum sample size using r = .45: `r ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, **two**-tailed. 
- Minimum sample size using lower CI r = .40: `r ceiling(pwr.r.test(n = NULL, r = .40, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, **two**-tailed.
- This suggested sample sizes using more common assumptions are `r round(ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)/ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n), 2)*100 - 100`% higher than Vahey et al.'s reported estimates.

## Forest plot

### From data in forest plot

```{r fig.height=2.5, fig.width=5}

data_vahey_forest <- data_vahey %>%
  drop_na() %>%
  select(article_abbreviated,
         n = n_forest_plot,
         r = mean_r_forest_plot_numeric,
         lower = ci_lower_forest_plot_numeric,
         upper = ci_upper_forest_plot_numeric) %>%
  bind_rows(tibble(article_abbreviated = "Meta",
                   n = 35, # arbitrary number to make the diamond an appropriate size
                   r = .45,
                   lower = .23,
                   upper = .67)) %>%
  mutate(size = n/sum(n),
         article_abbreviated = fct_relevel(article_abbreviated,
                                           "Carpenter et al. (2012)", 
                                           "Dawson et al. (2009)", 
                                           "Hooper et al, (2010)", 
                                           "Hussey & Barnes-Holmes (2012)", 
                                           "Kishita et al. (2014)", 
                                           "Kosnes et al. (2013)", 
                                           "Nicholson & Barnes-Holmes (2012a)", 
                                           "Nicholson & Barnes-Holmes (2012b)", 
                                           "Nicholson, Dempsey et al. (2013)", 
                                           "Nicholson, McCourt et al. (2013)", 
                                           "Parling et al. (2011)", 
                                           "Remue et al. (2013)", 
                                           "Timko et al. (2010, Study 1)", 
                                           "Vahey et al. (2009)", 
                                           "Vahey et al. (2010)",
                                           "Meta")) %>%
  mutate(` ` = paste(rep(" ", 30), collapse = " ")) %>%
  select(Article = article_abbreviated, ` `, r, lower, upper,  n, size) %>%
  round_df(2)

p <- 
  forestploter::forest(data       = select(data_vahey_forest, -size),
                       est        = data_vahey_forest$r,
                       lower      = data_vahey_forest$lower, 
                       upper      = data_vahey_forest$upper,
                       sizes      = 1/data_vahey_forest$size,
                       is_summary = c(rep(FALSE, nrow(data_vahey_forest)-1), TRUE),
                       ci_column  = 2,
                       ref_line   = 0,
                       theme      = custom_theme)

p

```

## Meta-analytic effect size 

The power analyses relied on the accuracy of the meta-analytic effect size. So, I then attempted to computationally reproduce the meta-analytic effect size from the effect sizes, confidence intervals, credibility intervals, and sample sizes reported in Vahey et al.'s forest plot and their description of their meta-analytic method in their manuscript. 

In an email, Vahey stated that they performed a Hunter & Schmidt meta analysis, following the guide of Field & Gillett (2010) and using their code.

On inspection, Field & Gillett (2010) make a distinction between the Hunter & Schmidt method, which is distinctive for reporting credibility intervals rather than confidence intervals, and the Hedges and colleaguesâ€™ method, which is distinctive for using Fisher's r-to-z transformations. 

Vahey et al. (2015) do not report applying any transformations to their data. However, Vahey et al.'s (2015) individual effect sizes in their forest plot have asymmetric confidence intervals. This implies that a transformation was performed. It is therefore possible that Vahey et al. combined the two methods (and code) provided by Field & Gillett (2010). I first fit a Hunter & Schmidt method, then a combined Hunter & Schmidt and Hedges' and colleagues method.  

### Hunter & Schmidt method

Information about implementation of Hunter & Schmidt-style meta-analysis in metafor from [here](http://www.metafor-project.org/doku.php/tips:hunter_schmidt_method).

```{r}

# calculate variances
data_recalculated_variance <- 
  escalc(measure = "COR", 
         ri      = mean_r_forest_plot_numeric, 
         ni      = n_forest_plot, 
         data    = data_vahey, 
         vtype   = "AV") %>%
  drop_na() %>%
  select(article, article_abbreviated,
         yi, vi, ni = n_forest_plot) %>%
  mutate(lower = yi - sqrt(vi)*1.96,
         upper = yi + sqrt(vi)*1.96)

# fit model
fit_reproduced <- 
  rma(yi      = yi, 
      vi      = vi, 
      weights = ni, # Hunter Schmidt method requires weighting by N
      method  = "HS", # Hunter Schmidt method
      data    = data_recalculated_variance,
      slab    = article_abbreviated)

predictions_reproduced <- 
  predict(fit_reproduced) %>%
  as.data.frame() %>%
  select(-se) %>%
  round_df(2)

predictions_reproduced %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# plot
data_forest <- data_recalculated_variance %>%
  drop_na() %>%
  select(article_abbreviated,
         n = ni,
         r = yi,
         lower,
         upper) %>%
  bind_rows(tibble(article_abbreviated = "Meta",
                   n = 35, # arbitrary number to make the diamond an appropriate size
                   r = predictions_reproduced$pred,
                   lower = predictions_reproduced$pi.lb,
                   upper = predictions_reproduced$pi.ub)) %>%
  mutate(size = n/sum(n),
         article_abbreviated = fct_relevel(article_abbreviated,
                                           "Carpenter et al. (2012)", 
                                           "Dawson et al. (2009)", 
                                           "Hooper et al, (2010)", 
                                           "Hussey & Barnes-Holmes (2012)", 
                                           "Kishita et al. (2014)", 
                                           "Kosnes et al. (2013)", 
                                           "Nicholson & Barnes-Holmes (2012a)", 
                                           "Nicholson & Barnes-Holmes (2012b)", 
                                           "Nicholson, Dempsey et al. (2013)", 
                                           "Nicholson, McCourt et al. (2013)", 
                                           "Parling et al. (2011)", 
                                           "Remue et al. (2013)", 
                                           "Timko et al. (2010, Study 1)", 
                                           "Vahey et al. (2009)", 
                                           "Vahey et al. (2010)",
                                           "Meta")) %>%
  mutate(` ` = paste(rep(" ", 30), collapse = " ")) %>%
  select(Article = article_abbreviated, ` `, r, lower, upper, n, size) %>%
  round_df(2)

p_reproduced <- 
  forestploter::forest(data       = select(data_forest, -size),
                       est        = data_forest$r,
                       lower      = data_forest$lower, 
                       upper      = data_forest$upper,
                       sizes      = 1/data_forest$size,
                       is_summary = c(rep(FALSE, nrow(data_forest)-1), TRUE),
                       ci_column  = 2,
                       ref_line   = 0,
                       theme      = custom_theme)

p_reproduced

```

- Individual estimates *are* the same as Vahey et al.'s forest plot.
- Individual estimates' confidence intervals *are not* the same as Vahey et al.'s forest plot.
- Meta estimate *is not* the same as Vahey et al.'s forest plot.
- Meta confidence intervals *are* same as Vahey et al.'s forest plot.
- Meta prediction intervals *are not* the same as Vahey et al.'s credibility intervals in their forest plot.

### Combined Hunter & Schmidt and Hedges' and colleagues method

Fisher's r-to-z transformation and back transformation, with HS estimator. No Overton (1998) transformation.

```{r}

# calculate variances
data_recalculated_variance_transformed <- 
  escalc(measure = "ZCOR", 
         ri      = mean_r_forest_plot_numeric, 
         ni      = n_forest_plot, 
         data    = data_vahey, 
         vtype   = "AV") %>%
  drop_na() %>%
  select(article, article_abbreviated,
         yi, vi, ni = n_forest_plot) %>%
  mutate(lower = yi - sqrt(vi)*1.96,
         upper = yi + sqrt(vi)*1.96)

# fit model
fit_reproduced_transformed <- 
  rma(yi      = yi, 
      vi      = vi, 
      weights = ni, # Hunter Schmidt method requires weighting by N
      method  = "HS", # Hunter Schmidt method
      data    = data_recalculated_variance_transformed,
      slab    = article_abbreviated)

predictions_reproduced_transformed <- 
  predict(fit_reproduced_transformed) %>%
  as.data.frame() %>%
  select(-se) %>%
  round_df(2)

predictions_reproduced_transformed %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# plot
data_forest_transformed <- data_recalculated_variance_transformed %>%
  select(article_abbreviated,
         n = ni,
         r = yi,
         lower,
         upper) %>%
  bind_rows(tibble(article_abbreviated = "Meta",
                   n = 35, # arbitrary number to make the diamond an appropriate size
                   r = predictions_reproduced$pred,
                   lower = predictions_reproduced$pi.lb,
                   upper = predictions_reproduced$pi.ub)) %>%
  mutate(r = transf.ztor(r),
         lower = transf.ztor(lower),
         upper = transf.ztor(upper)) %>%
  mutate(size = n/sum(n),
         article_abbreviated = fct_relevel(article_abbreviated,
                                           "Carpenter et al. (2012)", 
                                           "Dawson et al. (2009)", 
                                           "Hooper et al, (2010)", 
                                           "Hussey & Barnes-Holmes (2012)", 
                                           "Kishita et al. (2014)", 
                                           "Kosnes et al. (2013)", 
                                           "Nicholson & Barnes-Holmes (2012a)", 
                                           "Nicholson & Barnes-Holmes (2012b)", 
                                           "Nicholson, Dempsey et al. (2013)", 
                                           "Nicholson, McCourt et al. (2013)", 
                                           "Parling et al. (2011)", 
                                           "Remue et al. (2013)", 
                                           "Timko et al. (2010, Study 1)", 
                                           "Vahey et al. (2009)", 
                                           "Vahey et al. (2010)",
                                           "Meta")) %>%
  mutate(` ` = paste(rep(" ", 30), collapse = " ")) %>%
  select(Article = article_abbreviated, ` `, r, lower, upper, n, size) %>%
  round_df(2)

p_forest_transformed <- 
  forestploter::forest(data       = select(data_forest_transformed, -size),
                       est        = data_forest_transformed$r,
                       lower      = data_forest_transformed$lower, 
                       upper      = data_forest_transformed$upper,
                       sizes      = 1/data_forest_transformed$size,
                       is_summary = c(rep(FALSE, nrow(data_forest_transformed)-1), TRUE),
                       ci_column  = 2,
                       ref_line   = 0,
                       theme      = custom_theme)

p_forest_transformed

```

- Individual estimates *are* the same as Vahey et al.'s forest plot.
- Individual estimates' confidence intervals *are* the same as Vahey et al.'s forest plot.
- Meta estimate *is not* the same as Vahey et al.'s forest plot.
- Meta confidence intervals *are not* same as Vahey et al.'s forest plot.
- Meta prediction intervals *are not* the same as Vahey et al.'s credibility intervals in their forest plot.

### Summary

Vahey et al.'s meta CIs may have come from a meta analysis of native weighted r estimates, whereas their individual studies' confidence intervals may have come from a meta analysis of weighted r estimates transformed using Fisher's r-to-z prior to analysis and then back-transformed for plotting. 

It is unclear where their meta estimate or prediction/credibility intervals come from, they could not be reproduced.

## Weighted-average effect sizes reported in forest plot

Next, I attempted to reproduce the weighted-mean effect sizes reported in Vahey et al.'s forest plot from the individual effect sizes they reported in their supplementary materials. 

Vahey et al. reported that they weighted by degrees of freedom, and I do the same.

I noted that some of the confidence intervals in Vahey et al.'s forest plot are asymmetrical around the point estimate. This is uncommon and not accounted for by Vahey et al. detailing of how they calculated the effect sizes and their confidence intervals. However, I take them at face value as they are the most detailed data available to work from. 

```{r fig.height=4.5, fig.width=4.5}

# recalculate and compare
data_weighted_effect_sizes <- data_vahey %>%
  select(article = article_abbreviated, 
         r_supplementary, 
         df_supplementary) %>%
  group_by(article) %>%
  mutate(mean_df = mean(df_supplementary)) %>%
  ungroup() %>%
  mutate(r_weighted_by_df = r_supplementary*(df_supplementary/mean_df)) %>%
  group_by(article) %>%
  summarize(r_recalculated_weighted_mean = mean(r_weighted_by_df)) %>%
  left_join(data_vahey %>% 
              select(article = article_abbreviated, 
                     mean_r_forest_plot_numeric) %>%
              drop_na(),
            by = "article") %>%
  mutate(congruence = ifelse(janitor::round_half_up(r_recalculated_weighted_mean, 2) == 
                               janitor::round_half_up(mean_r_forest_plot_numeric, 2), "Congruent", "Incongruent"),
         congruence = fct_relevel(congruence, "Incongruent", "Congruent"),
         difference = r_recalculated_weighted_mean - mean_r_forest_plot_numeric)

# plot
ggplot(data_weighted_effect_sizes, aes(mean_r_forest_plot_numeric, r_recalculated_weighted_mean)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(0.35, 0.75) +
  ylim(0.35, 0.75) +
  xlab("Reported in Vahey et al.'s (2015)\nforest plot") +
  ylab("Recalculated from Vahey et al.'s (2015)\nsupplementary materials")

# table
data_weighted_effect_sizes %>%
  summarize(n_incongruent = sum(ifelse(congruence == "Incongruent", TRUE, FALSE)),
            percent_incongruent = round(mean(ifelse(congruence == "Incongruent", TRUE, FALSE)), 2)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_weighted_effect_sizes %>%
  filter(difference != 0) %>%
  select(article, r_recalculated_weighted_mean, mean_r_forest_plot_numeric, difference, congruence) %>%
  round_df(2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
  
```

```{r fig.height=4.5, fig.width=4.5}

# plot
data_weighted_effect_sizes %>%
  escalc(measure = "COR",
         ri = r_recalculated_weighted_mean,
         ni = n_forest_plot,
         data = .,
         slab = article,
         digits = 8) %>%
  ggplot(aes(ci_lower_forest_plot, yi - vi*1.96)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(-0.3, 0.7) +
  ylim(-0.3, 0.7) +
  xlab("Reported in Vahey et al.'s (2015)\nforest plot") +
  ylab("Recalculated from Vahey et al.'s (2015)\nsupplementary materials") +
  ggtitle("CI lower")

data_weighted_effect_sizes %>%
  mutate(r_recalculated_weighted_mean_adjusted = r_recalculated_weighted_mean - (r_recalculated_weighted_mean*(1-r_recalculated_weighted_mean^2)) / (2*(n_forest_plot-3))) %>%
  escalc(measure = "ZCOR",
         ri = r_recalculated_weighted_mean_adjusted,
         ni = n_forest_plot,
         data = .,
         slab = article,
         digits = 8) %>%
  ggplot(aes(ci_lower_forest_plot, yi - vi*1.96)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(aes(color = congruence, shape = congruence), size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(-0.3, 0.8) +
  ylim(-0.3, 0.8) +
  xlab("Reported in Vahey et al.'s (2015)\nforest plot") +
  ylab("Recalculated from Vahey et al.'s (2015)\nsupplementary materials")

```

CORRISPONDANCE IS BETTER WITHOUT TRANSFORMATIONS. WHAT ARE THE IMPLICATIONS FOR DECODING WHAT VAHEY ACTUALLY REPORTED? IT SUGGESTS THAT WHAT HE REPORTS AS WEIGHTED MEANS IN HIS SUPPLEMENTARY 


The weighted effect sizes reported in Vahey et al.'s forest plot could not be computationally reproduced from the individual effect sizes and degrees of freedom reported in their supplementary materials. 

Many values were congruent, but a minority differed (k = 2 [13%]). Table includes those that differed.

## Extraction and conversion of individual effect sizes 

Only a subset of effect sizes were reextracted and reconverted to Pearson's r. This was for a variety of reasons (note that sometimes multiple reasons applied to a given effect size):

- Vahey et al. appear to have treated the $\eta^2_p$ effect size reported in original papers as if it was equivalent to $\eta^2$, which it is not: (a) $\eta^2$ has a relatively simpler mathematical transformation to Pearson's r, which Vahey et al. appear to have applied to $\eta^2_p$, which is not possible to convert to Pearson's r due to the fact that it is a partial correlation (e.g., Hooper et al., 2010; Hussey & Barnes-Holmes, 2012).
- In some cases, effect sizes reported in Vahey et al.'s supplementary materials did not refer to effect sizes that were reported in the original article (e.g., Timko et al., 2010; Study 1, correlation between overall D score	and DASS-total).
- In some cases, effect sizes referred to analyses where mean IRAP D scores were the Dependent Variable rather than the Independent Variable (e.g., Kosnes et al., 2013, Parling et al., 2012; Hussey et al., 2012; Timko et al., 2010; Study 1).
- Some were not reported in sufficient detail in the original paper to allow for the calculation of an effect size, and contact with the original authors did not result in data/result being provided (e.g., Parling et al., 2012).
- Vahey et al. included a large number of non-zero IRAP effects (i.e., a reaction time differential between block types) as effect sizes. However, it should be noted that the presence of an IRAP effect in isolation, without reference to an external criterion variable, by definition cannot provide evidence for the IRAP's (clinical) criterion validity. As such, these effects were not considered for reextraction and checking, given that (a) this extraction work was labor intensive, and (b) these effect sizes would not later be employed in the updated meta-analysis for the aforementioned reason.

It is important to note that my goal here was not to account for the accuracy of every effect size, but to (a) demonstrate the difficulty of even assessing how effect sizes were extracted and calculated, and (b) illustrating that a number of errors have been made in the effect sizes it was possible to reextract and reconvert (agnostic to whether the ones I have not managed to extract and convert are correct or incorrect). That is, highlighting errors in the extraction and conversion of some effect sizes is sufficient to make the point that issues exist.  

```{r fig.height=4.5, fig.width=4.5}

# get effect sizes
data_individual_effect_sizes <- read.csv("../data/data_reextracted_and_annotated.csv") %>%
  # consider only those effect sizes employed in vahey et al.'s meta-analysis
  filter(es_included_in_vahey_meta == TRUE) %>%
  # create an simplified effect size identifier
  group_by(article) %>%
  mutate(es_number = row_number()) %>%
  ungroup() %>%
  select(article, es_number, r_vahey, r_from_paper, n_from_paper) 

# table
data_individual_effect_sizes %>%
  summarize(n_reextracted = sum(!is.na(r_from_paper)),
            percent_reextracted = round(sum(!is.na(r_from_paper))/n(), 3)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

# assess congruence
data_individual_effect_sizes_congruence <- data_individual_effect_sizes %>%
  na.omit() %>%
  mutate(congruence = ifelse(round(r_from_paper, 2) == round(r_vahey, 2), "Congruent", "Incongruent"),
         congruence = fct_relevel(congruence, "Incongruent", "Congruent"),
         difference = round(r_from_paper - r_vahey, 2))

# plot
ggplot(data_individual_effect_sizes_congruence, aes(r_vahey, r_from_paper)) +
  geom_abline(slope = 1, linetype = "dotted") +
  geom_point(aes(color = congruence, shape = congruence), size = 2.25) +
  theme_classic() +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + 
  theme(legend.position = c(0.25, 0.85),
        legend.title = element_blank(),
        legend.box.background = element_rect(colour = "black")) +
  xlim(0, 0.7) +
  ylim(0, 0.7) +
  xlab("Reported in Vahey et al.'s (2015)\nsupplementary materials") +
  ylab("Reextracted from original articles")

# table
data_individual_effect_sizes_congruence %>%
  summarize(n_incongruent = sum(ifelse(congruence == "Incongruent", TRUE, FALSE)),
            percent_incongruent = round(mean(ifelse(congruence == "Incongruent", TRUE, FALSE)), 2)*100) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

data_individual_effect_sizes_congruence %>%
  filter(difference != 0) %>%
  select(article, r_vahey, r_from_paper, difference) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)

```

Table includes those that differed.

## Critique of Vahey et al.'s inclusion and exclusion strategy 

**[notes on narrative, some coding still needed?]**

- how many ES were included by vahey, how many did they report considering?
- how many did i find.
- what were vahey's exclusion criteria?
- how many could be excluded using criterion 1: non-clinically relevant?
  - and, how many of vahey's could also be excluded as non-clinically relevant?
  - i.e., evidence of inconsistent application of inclusion/exclusion criteria.
- how many could be excluded using criterion 2: a priori predictable?
  - criterion seems inherently problematic, strong risk of post hoc bias. no blinding.
  - best I could do was re-rate all ES (only those scored as clinically relevant) for their predictability (rated by me and another researcher).
  - evidence from our ratings that many effects may have been predictable. seems very subjective - highlighting this point is useful at least.
  - evidence from Vahey et al.'s choices of what to include and not include seem less defensible however. Eg vahey et al 2009; comparisons between Nicholson et al. studies.

# New meta-analysis

- In attempting to determine whether Cahey et al.'s extracted effect sizes were accurately done, I realized that they made many mistakes of extraction or conversion, but more importantly many questionable decisions about which effect sizes they elected to include (e.g., presence of IRAP effects without reference to an external variable) or exclude (many effects that would seem to meet the criterion of being clinically relevant, but which were smaller in magnitude). I therefore elected to conduct a new meta-analysis using all clinically-relevant extracted effect sizes. I excluded the following from the extracted effect sizes:

- Mere IRAP effects without reference to an external variable that can provide criterion validity. By analogy, the fact that participants mean scores on on a self-report scale or a Stroop task tells us nothing about these measures' criterion validity, which is by definition assessed in relation to other external measures.  
- Effect sizes derived from analyses that employed IRAP effects as the DV. The final conclusion of the article is that the IRAP shows promise as a tool for clinical assessment, i.e., that it can be used to predict group membership. Predicting the mean IRAP effect on the basis of known groups is of relatively little utility. Moreover, this analytic mistake of confusing the IV and DV when attempting to provide evidence for a task's validity has been well documented elsewhere as a threat to research findings (e.g., Fried et al REF).
- Non-clinically relevant effects, using Vahey et al.'s definition (below) and scored by two independent raters, as in Vahey et al. I retained only effects that were rated by both raters as clinically relevant. 

From Vahey et al (2015): *"To be included within the current meta-analysis a given statistical effect must have described the co-variation of an IRAP effect with a corresponding clinically-focused criterion variable. To qualify as clinically-focused, the IRAP and criterion variables must have been deemed to target some aspect of a condition included in a major psychiatric diagnostic scheme such as the Diagnostic and Statistical Manual of Mental Disorders (DSM-5, 2013). ... The authors decided whether the responses measured by a given IRAP trial-type should co-vary with a specific criterion variable by consulting the relevant empirical literature. In the absence of such evidence, the authors strictly excluded even substantial statistical effects between IRAP effects and accompanying variables from the current meta-analysis."* Note that Vahey et al. did not provide citations of relevant literature that support the inclusion of any given effect.

```{r}

data_reextracted <- read.csv("../data/data_reextracted_and_annotated.csv") %>%
  # mark each effect as clinically relevant only if both raters scored it as such
  rowwise() %>%
  mutate(clinically_relevant = as.logical(max(c(clinically_relevant_criterion_rater_1,
                                                clinically_relevant_criterion_rater_2)))) %>%
  ungroup()

```

## Inter-rater reliability of inclusions

(i.e., of coding of effects as clinically relevant)

Vahey et al. do not report their rate of inter-rater reliability in the coding of effects as meeting inclusion/exclusion criteria.

I considered only the articles from which Vahey et al. extracted from, i.e., `r nrow(filter(data_reextracted, es_included_in_vahey_meta == TRUE))` effect sizes taken from `r nrow(distinct(data_reextracted, article))` articles, from an original pool of 46 articles. They did not report which or how many effect sizes were excluded, or the complete list of the original pool of articles. 

My reextraction found at least `r nrow(data_reextracted)` effect sizes in these `r nrow(distinct(data_reextracted, article))` articles. After exclusions, `r nrow(filter(data_reextracted, clinically_relevant == TRUE & problematic_analysis == FALSE))` effect sizes remained for inclusion in the new meta-analysis.

```{r}

data_raters <- data_reextracted %>%
  select(clinically_relevant_criterion_rater_1, clinically_relevant_criterion_rater_2)

data_raters %>%
  mutate(agreement = ifelse(clinically_relevant_criterion_rater_1 == clinically_relevant_criterion_rater_2, TRUE, FALSE)) %>%
  summarize(interrater_percent_agreement = round(mean(agreement, na.rm = TRUE), 1)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = FALSE)
  
kappa2(data_raters, "unweighted")

```

## Exclude non-clincially relevant and problematic

Details of the meta-analytic strategy, which used more up to date methods than Vahey et al.:

- Weighting by inverse variance rather than sample size, as sample size provides a poorer proxy of measurement error. this is now more standard practice in meta-analysis.
- Fishers r-to-z transformations to deal with ceiling effects on confidence intervals. 
- REML estimator function.
- Multilevel meta-analysis model rather than weighted-averaging of effect sizes (random intercept for source article; REF).

```{r fig.height=6, fig.width=5}

# exclusions and transformations
data_for_meta_new <- data_reextracted %>%
  filter(problematic_analysis == FALSE & clinically_relevant == TRUE) %>%
  mutate(variables = paste(irap_variable, criterion_variable, sep = "-")) %>%
  dplyr::select(article, variables, r_from_paper, n_from_paper, authors_include_bh) %>%
  na.omit() %>%
  escalc(measure = "ZCOR", 
         ri = r_from_paper, 
         ni = n_from_paper,
         data = ., 
         slab = paste(article, variables), 
         digits = 8) %>%
  rename(ni = n_from_paper) %>%
  dplyr::select(article, variables, yi, vi, ni, authors_include_bh) %>%
  na.omit() %>%
  group_by(article) %>%
  mutate(es_number = row_number()) %>%
  ungroup()

# fit 
fit_new <- 
  rma.mv(yi     = yi, 
         V      = vi, 
         random = ~ 1 | article, 
         method = "REML", 
         data   = data_for_meta_new,
         slab   = paste(article, es_number))

# save to disk for pdf plots
write_rds(fit_new, "models/fit_new.rds")

# make predictions 
predictions_new <-
  predict(fit_new, digits = 5) %>%
  as.data.frame() %>%
  gather(parameter, estimate) %>%
  mutate(parameter = dplyr::recode(parameter,
                                   "pred" = "Meta analysed r",
                                   "ci.lb" = "95% CI lower",
                                   "ci.ub" = "95% CI upper",
                                   "pi.lb" = "95% CR lower",
                                   "pi.ub" = "95% CR upper")) %>%
  filter(parameter != "se") %>%
  mutate(estimate = transf.ztor(estimate)) %>%
  round_df(2)

# caterpillar plot is more useful when k of effect sizes is large
metafor::forest(transf.ztor(data_for_meta_new$yi), 
                data_for_meta_new$vi,
                xlab = substitute(paste("Pearson's ", italic('r'))),
                subset = order(transf.ztor(data_for_meta_new$yi)),        ### order by size of yi
                slab = NA, 
                annotate = FALSE, ### remove study labels and annotations
                efac = 0,                  ### remove vertical bars at end of CIs
                pch = 19,                  ### changing point symbol to filled circle
                col = "gray40",            ### change color of points/CIs
                psize = 1.5,                 ### increase point size
                cex.lab = 1, 
                cex.axis = 1,   ### increase size of x-axis title/labels
                lty = c("solid","blank"),  ### remove horizontal line at top of plot
                xlim = c(-1.1, 1.1),    ### adjust horizontal plot region limits
                at = c(-1, -.5, 0, .5, 1))        
## draw points one more time to make them easier to see
points(sort(transf.ztor(data_for_meta_new$yi)), 
       nrow(data_for_meta_new):1, 
       pch = 19, 
       cex = 1)
## add meta effect size
addpoly(fit_new, 
        mlab = "", 
        annotate = FALSE, 
        addcred = TRUE,
        cex = 1)
## add text
text(-1, -2, "RE model", pos = 4, offset = 0, cex = 1)

# summarize results
meta_effect_new <- 
  paste0("k = ", fit_new$k, ", r = ", predictions_new$estimate[1],
         ", 95% CI [", predictions_new$estimate[2], ", ", predictions_new$estimate[3], "]",
         ", 95% CR [", predictions_new$estimate[4], ", ", predictions_new$estimate[5], "]",
         ", p = ", signif(2*pnorm(-abs(fit_new$zval)), digits = 1))  # exact p value from z score

# NB I2 is not trivial for multilevel models
meta_heterogeneity_new <-
  paste0("Q(df = ", fit_new$k - 1, ") = ", round(fit_new$QE, 2),
         ", p = ", ifelse(fit_new$QEp < 0.0001, "< .0001", as.character(round(fit_new$QEp, 4))),
         ", tau^2 = ", round(fit_new$tau2, 2))

```

Meta effect: `r meta_effect_new`.

Heterogeneity: `r meta_heterogeneity_new`.

- Note that this result is still known to be biased upward. Some papers were explicit that they reported only significant results in sufficient detail to calculate effect sizes (e.g., Parling et al., 2012).

## Updated power analyses

- Minimum sample size using r = `r predictions_new$estimate[1]` = `r ceiling(pwr.r.test(n = NULL, r = predictions_new$estimate[1], sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, *two*-tailed. 
- Minimum sample size using lower CI r = `r predictions_new$estimate[2]` = `r ceiling(pwr.r.test(n = NULL, r = predictions_new$estimate[2], sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)` participants to have 80% power, alpha = .05, *two*-tailed.
- This suggested sample sizes are more than `r round(ceiling(pwr.r.test(n = NULL, r = predictions_new$estimate[1], sig.level = 0.05, power = 0.8, alternative = "two.sided")$n)/ceiling(pwr.r.test(n = NULL, r = .45, sig.level = 0.05, power = 0.8, alternative = "greater")$n), 0)` times that recommended by Vahey et al.


